{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spellchecker in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (0.4)\n",
      "Requirement already satisfied: setuptools in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (from spellchecker) (65.5.0)\n",
      "Requirement already satisfied: inexactsearch in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (from spellchecker) (1.0.2)\n",
      "Requirement already satisfied: soundex>=1.0 in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (from inexactsearch->spellchecker) (1.1.3)\n",
      "Requirement already satisfied: silpa-common>=0.3 in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (from inexactsearch->spellchecker) (0.3)\n",
      "Requirement already satisfied: langdetect in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (1.0.7)\n",
      "Requirement already satisfied: six in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Requirement already satisfied: demoji in /Users/martjebuss/Documents/neuefische/repos/capstone_chat-gpt/.venv/lib/python3.11/site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spellchecker\n",
    "!pip install langdetect\n",
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/martjebuss/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/martjebuss/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from spellchecker import SpellChecker\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import demoji\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df\n",
    "df = pd.read_csv(\n",
    "    \"../data/ChatGPT-play-reviews.csv\",\n",
    "    encoding=\"utf-8\", parse_dates=[\"at\", \"repliedAt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('reviewCreatedVersion', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new date features from at\n",
    "df['at_ymd'] = df['at'].dt.strftime('%D')\n",
    "# Create new column for year-quarter\n",
    "df['at_q'] = df['at'].dt.quarter\n",
    "# Create new column for year-month\n",
    "df['at_ym'] = df['at'].dt.strftime('%Y-%m')\n",
    "# Create new column for month\n",
    "df['at_m'] = df['at'].dt.strftime('%B')\n",
    "# Create new column for year-month\n",
    "df['at_wd'] = df['at'].dt.strftime('%A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewId             0\n",
       "userName             1\n",
       "content              0\n",
       "score                0\n",
       "thumbsUpCount        0\n",
       "at                   0\n",
       "replyContent     30524\n",
       "repliedAt        30524\n",
       "appVersion        4914\n",
       "at_ymd               0\n",
       "at_q                 0\n",
       "at_ym                0\n",
       "at_m                 0\n",
       "at_wd                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display number of missing values per column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Costumer Reviews: \n",
    "Remove URLs, emails, phone numbers & punctuations.\n",
    "Remove tags, emojis, symbols & pictographs.\n",
    "Remove stop words.\n",
    "Convert to lowercase and lemmatization.\n",
    "Duplicates removal.\n",
    "Spell checking.\n",
    "Non-English reviews removal.\n",
    "Remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"score_cat\"] = np.where(df.score == 5, \"positive\", np.where(df.score == 4, \"neutral\", \"negative\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove emojis and symbols, standardize mentions of ChatGPT and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "\n",
    "    # remove emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "         u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "         u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n",
    "         u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "         u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n",
    "                       \"]+\", re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "    dem = demoji.findall(text)\n",
    "    for item in dem.keys():\n",
    "        text = text.replace(item, '')\n",
    "    \n",
    "    # remove all characters that are not alphanumeric\n",
    "    #text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # remove symbols\n",
    "    symbol_pattern = re.compile(r'[@#$%^&*()_+{}\\[\\]\"\\<>,/\\\\|`~]+')\n",
    "    text = symbol_pattern.sub(r'', text)\n",
    "\n",
    "    # remove - \n",
    "    dash_pattern = re.compile(r'-+')\n",
    "    text = dash_pattern.sub(r'', text)\n",
    "\n",
    "    #split the string into separate tokens\n",
    "    tokens = re.split(r\"\\s+\",text)\n",
    "\n",
    "    # normalise all words into lowercase\n",
    "    text = \" \".join([t.lower() for t in tokens])\n",
    "\n",
    "    # standardize\n",
    "    text = text.replace(\"chat gpt\", \"chatgpt\")\n",
    "    text = text.replace(\"open ai\", \"openai\")\n",
    "\n",
    "    # return final list of tokens\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30866         yo tengo\n",
       "30867    thank chatgpt\n",
       "30868       1st review\n",
       "30869         just wow\n",
       "30870     ÿ™ÿ∑ÿ®ŸäŸÇ ÿßÿ≠ÿ™ÿ±ÿßŸÅ\n",
       "             ...      \n",
       "30951             Ô∏èÔ∏èÔ∏èÔ∏è\n",
       "30952             Ô∏èÔ∏èÔ∏èÔ∏è\n",
       "30953             Ô∏èÔ∏èÔ∏èÔ∏è\n",
       "30954                Ô∏è\n",
       "30955                5\n",
       "Name: content, Length: 90, dtype: object"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'].tail(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_text = \"Amaznig and extremely handy app for many uses.... ü§ç it's like an extension of one's fingers ‚≠êÔ∏è‚≠êÔ∏è. #ChatGPT Chat GPT OpenAI Open AI HTML Google\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amaznig and extremely handy app for many uses.... ü§ç it's like an extension of one's fingers ‚≠êÔ∏è‚≠êÔ∏è. #ChatGPT Chat GPT OpenAI Open AI HTML Google\n",
      "amaznig and extremely handy app for many uses.... it's like an extension of one's fingers Ô∏èÔ∏è. chatgpt chatgpt openai openai html google\n"
     ]
    }
   ],
   "source": [
    "#corrected_text = pre_process(test_text)\n",
    "#print(test_text)\n",
    "#print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_hashtags(text): \n",
    "#     hashtag_pattern = re.compile(r'#\\S+')\n",
    "#     return hashtag_pattern.sub('', text)\n",
    "# df['content'] = df['content'].apply(remove_hashtags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_duplicates = df.duplicated().sum()\n",
    "count_duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewId             0\n",
       "userName             1\n",
       "content              0\n",
       "score                0\n",
       "thumbsUpCount        0\n",
       "at                   0\n",
       "replyContent     30524\n",
       "repliedAt        30524\n",
       "appVersion        4914\n",
       "at_ymd               0\n",
       "at_q                 0\n",
       "at_ym                0\n",
       "at_m                 0\n",
       "at_wd                0\n",
       "score_cat            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(comment):\n",
    "    try:\n",
    "        return detect(comment)\n",
    "    except:\n",
    "        return 'unknown' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['detected_language'] = df['content'].apply(detect_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['en', 'ca', 'it', 'nl', 'fr', 'no', 'ro', 'af', 'da', 'sv', 'hr',\n",
       "       'so', 'et', 'pt', 'sq', 'tl', 'id', 'hu', 'sw', 'de', 'cy', 'fi',\n",
       "       'pl', 'es', 'sl', 'cs', 'lv', 'ml', 'unknown', 'sk', 'ta', 'tr',\n",
       "       'lt', 'vi', 'ar', 'hi', 'fa', 'ur', 'ru', 'bn', 'ne', 'ja', 'uk',\n",
       "       'el', 'zh-cn', 'ko', 'te', 'gu', 'mr', 'th', 'kn', 'he', 'bg',\n",
       "       'mk'], dtype=object)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['detected_language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21352 entries, 0 to 30923\n",
      "Data columns (total 16 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   reviewId           21352 non-null  object        \n",
      " 1   userName           21351 non-null  object        \n",
      " 2   content            21352 non-null  object        \n",
      " 3   score              21352 non-null  int64         \n",
      " 4   thumbsUpCount      21352 non-null  int64         \n",
      " 5   at                 21352 non-null  datetime64[ns]\n",
      " 6   replyContent       386 non-null    object        \n",
      " 7   repliedAt          386 non-null    datetime64[ns]\n",
      " 8   appVersion         18088 non-null  object        \n",
      " 9   at_ymd             21352 non-null  object        \n",
      " 10  at_q               21352 non-null  int32         \n",
      " 11  at_ym              21352 non-null  object        \n",
      " 12  at_m               21352 non-null  object        \n",
      " 13  at_wd              21352 non-null  object        \n",
      " 14  score_cat          21352 non-null  object        \n",
      " 15  detected_language  21352 non-null  object        \n",
      "dtypes: datetime64[ns](2), int32(1), int64(2), object(11)\n",
      "memory usage: 2.7+ MB\n"
     ]
    }
   ],
   "source": [
    "lang = df[df['detected_language'] == \"en\"]\n",
    "lang.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"../data/chatgpt_short_clean_all_languages.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into short (review-wise) and long format (sentence-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index                                            content  score score_cat  \\\n",
      "34      5                                 seems to work now.      3  negative   \n",
      "35      5                 app seems nice but has two issues.      3  negative   \n",
      "36      5  the website mentions that users can enable voi...      3  negative   \n",
      "37      5  also when using voice dictation to compose a p...      3  negative   \n",
      "\n",
      "   detected_language  \n",
      "34                en  \n",
      "35                en  \n",
      "36                en  \n",
      "37                en  \n"
     ]
    }
   ],
   "source": [
    "# Function to split text into sentences\n",
    "def split_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Split sentences and create a new DataFrame in long format\n",
    "new_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    sentences = split_sentences(row['content'])\n",
    "    score = row['score']\n",
    "    score_cat = row['score_cat']\n",
    "    detected_language = row['detected_language']\n",
    "    for sentence in sentences:\n",
    "        new_rows.append({'index': index, 'content': sentence, 'score': score, 'score_cat': score_cat, 'detected_language': detected_language})\n",
    "\n",
    "df_long = pd.DataFrame(new_rows)\n",
    "\n",
    "# Print the resulting DataFrame in long format\n",
    "print(df_long[df_long['index'] == 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df[df['detected_language'] == \"en\"]\n",
    "df_long_en = df_long[df_long['detected_language'] == \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "exceptions = [\"chatgpt\", \"openai\", \"gpt\", \"html\", \"css\", \"javascript\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text): \n",
    "    words = text.split()\n",
    "    corrected_text = []\n",
    "    for word in words:\n",
    "        if word in exceptions:\n",
    "            corrected_text.append(word)\n",
    "        else: \n",
    "            corrected_word = spell.correction(word)\n",
    "            if corrected_word is not None: \n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "    corrected_text = \" \".join(corrected_text)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_text2 = correct_spelling(corrected_text)\n",
    "# print(test_text)\n",
    "# print(corrected_text)\n",
    "# print(corrected_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x1/pr8ngbs166d7stxkx1fr_b480000gn/T/ipykernel_87507/367844575.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_en['content'] = df_en['content'].apply(correct_spelling)\n",
      "/var/folders/x1/pr8ngbs166d7stxkx1fr_b480000gn/T/ipykernel_87507/367844575.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_long_en['content'] = df_long_en['content'].apply(correct_spelling)\n"
     ]
    }
   ],
   "source": [
    "df_en['content'] = df_en['content'].apply(correct_spelling)\n",
    "df_long_en['content'] = df_long_en['content'].apply(correct_spelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.to_csv(\"../data/chatgpt_short_clean_en.csv\")\n",
    "df_long_en.to_csv(\"../data/chatgpt_long_clean_en.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
