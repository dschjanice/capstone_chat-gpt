{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try some feature engineering approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this notebook we are testing the Zero-Shot Classification and VADER’s Sentiment Intensity Analyzer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load useful libraries and df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/janice/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/janice/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df\n",
    "df = pd.read_csv(\n",
    "    \"../data/chatgpt_after_datacleaning.csv\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# var for review received response\n",
    "df[\"score_cat\"] = np.where(df.score == 5, \"positive\", np.where(df.score == 4, \"neutral\", \"negative\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "      <th>at_ymd</th>\n",
       "      <th>at_q</th>\n",
       "      <th>at_ym</th>\n",
       "      <th>at_m</th>\n",
       "      <th>at_wd</th>\n",
       "      <th>score_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>36b7f28e-151d-4b98-8a13-41bd017e0d25</td>\n",
       "      <td>Lin Cheng</td>\n",
       "      <td>catgut on Android is a solid app with seamless open server connectivity ensuring smooth interactions however it falls behind its Apple counterpart in features and updates The voice input can be prematurely triggered by pauses unlike on apple additionally the lack of a search function for previous messages is a drawback Despite these it remains a commendable app deserving a 4-5 star ratings With minor improvements it can match its Apple version</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2023-10-19 19:26:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2023.284</td>\n",
       "      <td>10/19/23</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-10</td>\n",
       "      <td>October</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2bc253b6-c804-47e9-b6f2-3a21027efab4</td>\n",
       "      <td>Alim</td>\n",
       "      <td>I've been using catgut for a while but I've just tested out the microphone speech recognition option for the first time and let's say... I'M COMPLETELY BLOWN away not seriously It literally puts ALL the expressions punctuation in the right place No matter how you talk it converts it without a problem It's amazing and I will probably will never type to catgut again Still though... that's some outstanding work Now we wait for voice responses from the both Hopefully...</td>\n",
       "      <td>5</td>\n",
       "      <td>139</td>\n",
       "      <td>2023-09-29 20:24:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2023.263</td>\n",
       "      <td>09/29/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-09</td>\n",
       "      <td>September</td>\n",
       "      <td>Friday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5f084727-ab85-40b3-bd42-a7a49502fc1f</td>\n",
       "      <td>Theo Healy</td>\n",
       "      <td>The catgut Android app has completely blown me away with its exceptional performance and versatility As an AI language model it consistently delivers impressive responses to my queries and provides insightful suggestions The ease of use and intuitive interface make chatting with catgut a delightful experience In all seriousness it's good It works I would still recommend using the browser thought as the app lacks a few features such as editing a message after it's sent</td>\n",
       "      <td>4</td>\n",
       "      <td>247</td>\n",
       "      <td>2023-07-28 10:29:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0.0023</td>\n",
       "      <td>07/28/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07</td>\n",
       "      <td>July</td>\n",
       "      <td>Friday</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5df90de5-b8e2-4dc2-b6ff-520aa3a25eae</td>\n",
       "      <td>Elliot Limberg</td>\n",
       "      <td>No subscription free and accurate unbiased answers No fresh data since September 2021 but that doesn't make this app less accurate I'm loving the fact it can even generate basic acid ART other than just text The text is excellent grammar precise and a great plethora of vocabulary I can't wait for the next update and to see what that brings</td>\n",
       "      <td>5</td>\n",
       "      <td>272</td>\n",
       "      <td>2023-07-30 19:38:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0.0023</td>\n",
       "      <td>07/30/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07</td>\n",
       "      <td>July</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bb66c666-865d-4a31-b27f-4933df3ff829</td>\n",
       "      <td>Phoebe Moraes</td>\n",
       "      <td>I use this app for learning languages which catgut is amazing at however there are a few things I'd like added 1. A search function Being able to search a word that's several tokens back would be great 2. Ability to highlight a word and search with googled 3. An AI voice that can read back chatgpt's outputs For language learning especially it'd be very useful to hear how certain words are pronounced But still great client for chatgpt! Can't wait for future improvements</td>\n",
       "      <td>4</td>\n",
       "      <td>126</td>\n",
       "      <td>2023-08-09 18:23:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0.0030</td>\n",
       "      <td>08/09/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-08</td>\n",
       "      <td>August</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30951</th>\n",
       "      <td>30951</td>\n",
       "      <td>a92cb6cd-62c6-492b-8d41-be11a3720fbd</td>\n",
       "      <td>Omin</td>\n",
       "      <td>❤️❤️❤️❤️</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-31 19:41:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/31/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07</td>\n",
       "      <td>July</td>\n",
       "      <td>Monday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30952</th>\n",
       "      <td>30952</td>\n",
       "      <td>e49cc721-c04b-4b01-b532-3285a9661a32</td>\n",
       "      <td>G G</td>\n",
       "      <td>❤️❤️❤️❤️</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-25 17:11:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/25/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07</td>\n",
       "      <td>July</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30953</th>\n",
       "      <td>30953</td>\n",
       "      <td>479f78e9-24fc-4609-9d0c-82eb07140327</td>\n",
       "      <td>Kanhaiya Lal Kanhaiya Lal</td>\n",
       "      <td>❣️❣️❣️❣️</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-19 02:20:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10/19/23</td>\n",
       "      <td>4</td>\n",
       "      <td>2023-10</td>\n",
       "      <td>October</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30954</th>\n",
       "      <td>30954</td>\n",
       "      <td>f6f1563d-a0cf-452b-b34c-d2e0fe34c0d0</td>\n",
       "      <td>SONU U S</td>\n",
       "      <td>i</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-25 19:35:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/25/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-07</td>\n",
       "      <td>July</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30955</th>\n",
       "      <td>30955</td>\n",
       "      <td>abba1974-2ef5-4a06-8a72-4323ff0134b9</td>\n",
       "      <td>Vishal Kadam</td>\n",
       "      <td>i</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-27 08:27:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>09/27/23</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-09</td>\n",
       "      <td>September</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30956 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                              reviewId  \\\n",
       "0               0  36b7f28e-151d-4b98-8a13-41bd017e0d25   \n",
       "1               1  2bc253b6-c804-47e9-b6f2-3a21027efab4   \n",
       "2               2  5f084727-ab85-40b3-bd42-a7a49502fc1f   \n",
       "3               3  5df90de5-b8e2-4dc2-b6ff-520aa3a25eae   \n",
       "4               4  bb66c666-865d-4a31-b27f-4933df3ff829   \n",
       "...           ...                                   ...   \n",
       "30951       30951  a92cb6cd-62c6-492b-8d41-be11a3720fbd   \n",
       "30952       30952  e49cc721-c04b-4b01-b532-3285a9661a32   \n",
       "30953       30953  479f78e9-24fc-4609-9d0c-82eb07140327   \n",
       "30954       30954  f6f1563d-a0cf-452b-b34c-d2e0fe34c0d0   \n",
       "30955       30955  abba1974-2ef5-4a06-8a72-4323ff0134b9   \n",
       "\n",
       "                        userName  \\\n",
       "0                      Lin Cheng   \n",
       "1                           Alim   \n",
       "2                     Theo Healy   \n",
       "3                 Elliot Limberg   \n",
       "4                  Phoebe Moraes   \n",
       "...                          ...   \n",
       "30951                       Omin   \n",
       "30952                        G G   \n",
       "30953  Kanhaiya Lal Kanhaiya Lal   \n",
       "30954                   SONU U S   \n",
       "30955               Vishal Kadam   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         content  \\\n",
       "0                                catgut on Android is a solid app with seamless open server connectivity ensuring smooth interactions however it falls behind its Apple counterpart in features and updates The voice input can be prematurely triggered by pauses unlike on apple additionally the lack of a search function for previous messages is a drawback Despite these it remains a commendable app deserving a 4-5 star ratings With minor improvements it can match its Apple version   \n",
       "1         I've been using catgut for a while but I've just tested out the microphone speech recognition option for the first time and let's say... I'M COMPLETELY BLOWN away not seriously It literally puts ALL the expressions punctuation in the right place No matter how you talk it converts it without a problem It's amazing and I will probably will never type to catgut again Still though... that's some outstanding work Now we wait for voice responses from the both Hopefully...   \n",
       "2       The catgut Android app has completely blown me away with its exceptional performance and versatility As an AI language model it consistently delivers impressive responses to my queries and provides insightful suggestions The ease of use and intuitive interface make chatting with catgut a delightful experience In all seriousness it's good It works I would still recommend using the browser thought as the app lacks a few features such as editing a message after it's sent   \n",
       "3                                                                                                                                          No subscription free and accurate unbiased answers No fresh data since September 2021 but that doesn't make this app less accurate I'm loving the fact it can even generate basic acid ART other than just text The text is excellent grammar precise and a great plethora of vocabulary I can't wait for the next update and to see what that brings   \n",
       "4      I use this app for learning languages which catgut is amazing at however there are a few things I'd like added 1. A search function Being able to search a word that's several tokens back would be great 2. Ability to highlight a word and search with googled 3. An AI voice that can read back chatgpt's outputs For language learning especially it'd be very useful to hear how certain words are pronounced But still great client for chatgpt! Can't wait for future improvements   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ...   \n",
       "30951                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ❤️❤️❤️❤️   \n",
       "30952                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ❤️❤️❤️❤️   \n",
       "30953                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ❣️❣️❣️❣️   \n",
       "30954                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          i   \n",
       "30955                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          i   \n",
       "\n",
       "       score  thumbsUpCount                   at replyContent repliedAt  \\\n",
       "0          4              5  2023-10-19 19:26:19          NaN       NaN   \n",
       "1          5            139  2023-09-29 20:24:38          NaN       NaN   \n",
       "2          4            247  2023-07-28 10:29:10          NaN       NaN   \n",
       "3          5            272  2023-07-30 19:38:37          NaN       NaN   \n",
       "4          4            126  2023-08-09 18:23:33          NaN       NaN   \n",
       "...      ...            ...                  ...          ...       ...   \n",
       "30951      5              0  2023-07-31 19:41:55          NaN       NaN   \n",
       "30952      5              0  2023-07-25 17:11:59          NaN       NaN   \n",
       "30953      5              0  2023-10-19 02:20:49          NaN       NaN   \n",
       "30954      5              0  2023-07-25 19:35:41          NaN       NaN   \n",
       "30955      5              0  2023-09-27 08:27:40          NaN       NaN   \n",
       "\n",
       "       appVersion    at_ymd  at_q    at_ym       at_m      at_wd score_cat  \n",
       "0      1.2023.284  10/19/23     4  2023-10    October   Thursday   neutral  \n",
       "1      1.2023.263  09/29/23     3  2023-09  September     Friday  positive  \n",
       "2        1.0.0023  07/28/23     3  2023-07       July     Friday   neutral  \n",
       "3        1.0.0023  07/30/23     3  2023-07       July     Sunday  positive  \n",
       "4        1.0.0030  08/09/23     3  2023-08     August  Wednesday   neutral  \n",
       "...           ...       ...   ...      ...        ...        ...       ...  \n",
       "30951         NaN  07/31/23     3  2023-07       July     Monday  positive  \n",
       "30952         NaN  07/25/23     3  2023-07       July    Tuesday  positive  \n",
       "30953         NaN  10/19/23     4  2023-10    October   Thursday  positive  \n",
       "30954         NaN  07/25/23     3  2023-07       July    Tuesday  positive  \n",
       "30955         NaN  09/27/23     3  2023-09  September  Wednesday  positive  \n",
       "\n",
       "[30956 rows x 16 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking at a sample of our df\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer to the training df\n",
    "vect = CountVectorizer().fit(df['content'].values.astype('U'))\n",
    "\n",
    "# transform the documents in the training df to a document-term matrix\n",
    "X_train_vectorized = vect.transform(df['content'].values.astype('U'))\n",
    "# print(\"X_train_vectorized: \", X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape = (30956,)\n",
      "Vocabulary length = 12872\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape = {}\".format(df['content'].shape))\n",
    "print(\"Vocabulary length = {}\".format(len(vect.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in 30.956 messages we found 12.872 different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abnormal', 200),\n",
       " ('aboard', 201),\n",
       " ('abort', 202),\n",
       " ('about', 203),\n",
       " ('above', 204),\n",
       " ('abraham', 205),\n",
       " ('absence', 206),\n",
       " ('absent', 207),\n",
       " ('absolute', 208),\n",
       " ('absolutely', 209),\n",
       " ('abstract', 210),\n",
       " ('absulitly', 211),\n",
       " ('abt', 212),\n",
       " ('abujiji', 213),\n",
       " ('abundantly', 214),\n",
       " ('abuse', 215),\n",
       " ('abused', 216),\n",
       " ('abusive', 217),\n",
       " ('abysmal', 218),\n",
       " ('ac', 219),\n",
       " ('acacia', 220),\n",
       " ('academia', 221),\n",
       " ('academic', 222),\n",
       " ('academically', 223),\n",
       " ('academics', 224),\n",
       " ('academy', 225),\n",
       " ('accelerate', 226),\n",
       " ('accelerated', 227),\n",
       " ('accent', 228),\n",
       " ('accentuate', 229),\n",
       " ('accept', 230),\n",
       " ('acceptable', 231),\n",
       " ('accepted', 232),\n",
       " ('accepting', 233),\n",
       " ('acces', 234),\n",
       " ('access', 235),\n",
       " ('accessed', 236),\n",
       " ('accessibility', 237),\n",
       " ('accessible', 238),\n",
       " ('accessing', 239)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at our vocabulary list (sorted alphabetically)\n",
    "# Does it look like you expected?\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[200:240]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "TF-IDF is short for **Term Frequency - Inverse Document Frequency**. \n",
    "\n",
    "It measure how important a word is to a document in a set of texts (in our case all SMS we collected). A frequent word in a document that is also frequent in the corpus is less important to a document than a frequent word in a document that is not frequent in the corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accessible', 10),\n",
       " ('account', 11),\n",
       " ('accuracy', 12),\n",
       " ('accurate', 13),\n",
       " ('across', 14),\n",
       " ('actually', 15),\n",
       " ('ad', 16),\n",
       " ('add', 17),\n",
       " ('added', 18),\n",
       " ('adding', 19),\n",
       " ('addition', 20),\n",
       " ('ads', 21),\n",
       " ('advanced', 22),\n",
       " ('advice', 23),\n",
       " ('after', 24),\n",
       " ('again', 25),\n",
       " ('age', 26),\n",
       " ('ago', 27),\n",
       " ('ai', 28),\n",
       " ('aids', 29)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the TfidfVectorizer to the training df specifiying a minimum document frequency of 30\n",
    "# This means a word should have been used in at least 30 reviews\n",
    "vect = TfidfVectorizer(min_df=30).fit(df['content'].values.astype('U'))\n",
    "\n",
    "# transform the documents in the training df to a document-term matrix\n",
    "X_train_vectorized = vect.transform(df['content'].values.astype('U'))\n",
    "\n",
    "# let's look of some of the words gathered with this method\n",
    "sorted(vect.vocabulary_.items(), key=lambda x: x[1])[10:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words appear in more than 20 text messages\n",
    "len(sorted(vect.vocabulary_.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check which words created the largest tfidf values for the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest tfidf:\n",
      "['exceeded' 'processing' 'consistently' 'versatility' 'powered' 'coherent'\n",
      " 'interactions' 'natural' 'however' 'whether']\n",
      "\n",
      "Largest tfidf: \n",
      "['youtube' 'fast' 'informative' 'information' 'size' 'incredible' 'in'\n",
      " 'smart' 'smooth' 'impressive']\n"
     ]
    }
   ],
   "source": [
    "# save all feature names == words in an array\n",
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "\n",
    "#sort for the column names according to highest tfidf value in the column\n",
    "sorted_tfidf_index = X_train_vectorized.toarray().max(0).argsort()\n",
    "\n",
    "# print words with highest and lowest tfidf values\n",
    "print(\"Smallest tfidf:\\n{}\\n\".format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print(\"Largest tfidf: \\n{}\".format(feature_names[sorted_tfidf_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming reduces a word to its stem. The result is less readable to humans, but makes the text more comparable across observations.\n",
    "\n",
    "For example, the words \"consult\", \"consultant\", \"consulting\", \" consultative\", \"consultants\" have the same stem **\"consult \"**.\n",
    "\n",
    "We will now add stemming as a preprocessing step to our workflow. The nltk PorterStemmer will generate the stems of the words. These features will be used in the CountVectorizer to create a matrix with the number of features (stemmed words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing stemmer and countvectorizer \n",
    "stemmer = nltk.PorterStemmer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "# tfidf_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in cv_analyzer(doc))\n",
    "\n",
    "# define CountVectorizer with stemming function \n",
    "stem_vectorizer = CountVectorizer(analyzer = stemmed_words)\n",
    "# stem_vectorizer = TfidfVectorizer(min_df=15, analyzer = stemmed_words)\n",
    "\n",
    "\n",
    "# Transform X_train\n",
    "df_content_stem_vectorized = stem_vectorizer.fit_transform(df['content'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text -  I've been using catgut for a while now and I'm genuinely impressed with its capabilities The way it generates text and engages in conversations is nothing short of remarkable The responses are coherent relevant and often feel like they're coming from a real person.ChatGPT's versatility is astonishing From answering factual questions to offering creative writing prompts it covers a wide range of topics effortlessly It's a fantastic tool for brainstorming getting quick explanations\n",
      "------------------------------\n",
      "Text after passing through build_analyzer -  ['ve', 'been', 'using', 'catgut', 'for', 'while', 'now', 'and', 'genuinely', 'impressed', 'with', 'its', 'capabilities', 'the', 'way', 'it', 'generates', 'text', 'and', 'engages', 'in', 'conversations', 'is', 'nothing', 'short', 'of', 'remarkable', 'the', 'responses', 'are', 'coherent', 'relevant', 'and', 'often', 'feel', 'like', 'they', 're', 'coming', 'from', 'real', 'person', 'chatgpt', 'versatility', 'is', 'astonishing', 'from', 'answering', 'factual', 'questions', 'to', 'offering', 'creative', 'writing', 'prompts', 'it', 'covers', 'wide', 'range', 'of', 'topics', 'effortlessly', 'it', 'fantastic', 'tool', 'for', 'brainstorming', 'getting', 'quick', 'explanations']\n",
      "------------------------------\n",
      "Text after stemming -  ['ve', 'been', 'use', 'catgut', 'for', 'while', 'now', 'and', 'genuin', 'impress', 'with', 'it', 'capabl', 'the', 'way', 'it', 'gener', 'text', 'and', 'engag', 'in', 'convers', 'is', 'noth', 'short', 'of', 'remark', 'the', 'respons', 'are', 'coher', 'relev', 'and', 'often', 'feel', 'like', 'they', 're', 'come', 'from', 'real', 'person', 'chatgpt', 'versatil', 'is', 'astonish', 'from', 'answer', 'factual', 'question', 'to', 'offer', 'creativ', 'write', 'prompt', 'it', 'cover', 'wide', 'rang', 'of', 'topic', 'effortlessli', 'it', 'fantast', 'tool', 'for', 'brainstorm', 'get', 'quick', 'explan']\n"
     ]
    }
   ],
   "source": [
    "r = 300\n",
    "sample_text = df['content'][r:r+1]\n",
    "print(\"Sample Text - \", sample_text[sample_text.index[0]])\n",
    "print(\"-\"*30)\n",
    "print(\"Text after passing through build_analyzer - \", cv_analyzer(sample_text[sample_text.index[0]]))\n",
    "print(\"-\"*30)\n",
    "print(\"Text after stemming - \",[stemmer.stem(w) for w in cv_analyzer(sample_text[sample_text.index[0]])])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try uncommenting the tfidf lines in the cell above, so instead of using CountVectorizer you can also use TfIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The same way we used stemming we can also apply lemmatization to our df.\n",
    "Lemmatization reduces variant forms to base form (eg. am, are, is --> be; car, cars, car's, cars' --> car).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "cv_analyzer = CountVectorizer().build_analyzer()\n",
    "# cv_analyzer = TfidfVectorizer(min_df=15).build_analyzer()\n",
    "\n",
    "def lemmatize_word(doc):\n",
    "    return (WNlemma.lemmatize(t) for t in cv_analyzer(doc))\n",
    "\n",
    "lemm_vectorizer = CountVectorizer(analyzer = lemmatize_word)\n",
    "# lemm_vectorizer = TfidfVectorizer(min_df=15, analyzer=lemmatize_word)\n",
    "\n",
    "# Transform X_train\n",
    "df_content_lemm_vectorized = lemm_vectorizer.fit_transform(df['content'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30956, 11832)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content_lemm_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text -  The app is good but the only problem is there are still a few bugs It also doesn't fix it's own mistake after clarifying it and even after the robot said that I was correct Only after asking it 2 times did it fix it ONLY once It kind of has troubles with the years so the creators should fix it\n",
      "------------------------------\n",
      "Text after passing through build_analyzer -  ['the', 'app', 'is', 'good', 'but', 'the', 'only', 'problem', 'is', 'there', 'are', 'still', 'few', 'bugs', 'it', 'also', 'doesn', 'fix', 'it', 'own', 'mistake', 'after', 'clarifying', 'it', 'and', 'even', 'after', 'the', 'robot', 'said', 'that', 'was', 'correct', 'only', 'after', 'asking', 'it', 'times', 'did', 'it', 'fix', 'it', 'only', 'once', 'it', 'kind', 'of', 'has', 'troubles', 'with', 'the', 'years', 'so', 'the', 'creators', 'should', 'fix', 'it']\n",
      "------------------------------\n",
      "Text after stemming -  ['the', 'app', 'is', 'good', 'but', 'the', 'only', 'problem', 'is', 'there', 'are', 'still', 'few', 'bug', 'it', 'also', 'doesn', 'fix', 'it', 'own', 'mistake', 'after', 'clarifying', 'it', 'and', 'even', 'after', 'the', 'robot', 'said', 'that', 'wa', 'correct', 'only', 'after', 'asking', 'it', 'time', 'did', 'it', 'fix', 'it', 'only', 'once', 'it', 'kind', 'of', 'ha', 'trouble', 'with', 'the', 'year', 'so', 'the', 'creator', 'should', 'fix', 'it']\n"
     ]
    }
   ],
   "source": [
    "r = 500\n",
    "sample_text = df['content'][r:r+1]\n",
    "print(\"Sample Text - \", sample_text[sample_text.index[0]])\n",
    "print(\"-\"*30)\n",
    "print(\"Text after passing through build_analyzer - \", cv_analyzer(sample_text[sample_text.index[0]]))\n",
    "print(\"-\"*30)\n",
    "print(\"Text after stemming - \",[WNlemma.lemmatize(t) for t in cv_analyzer(sample_text[sample_text.index[0]])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.nlp': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "9fb4e7a82a71b285d1d2fdbccfc7dcb00fac1863548a2573de3cd7a5b08c832d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
